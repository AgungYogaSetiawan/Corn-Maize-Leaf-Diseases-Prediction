# -*- coding: utf-8 -*-
"""Corn Maize Leaf Disease Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16vroV5lLsZIMYTduFHiYOXs9uKrw7zGc

### Import Required Dependencies
"""

import tensorflow as tf
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import os
from tensorflow.keras import models, layers

"""### Set Constant Value"""

IMAGE_SIZE = 256
BATCH_SIZE = 32
CHANNELS = 3

"""### Upload Image Dataset

We use image of corn maize leaf diseases
"""

dataset = tf.keras.preprocessing.image_dataset_from_directory(
    'Data Resize',
    batch_size=BATCH_SIZE,
    image_size=(IMAGE_SIZE, IMAGE_SIZE),
    seed=123,
    shuffle=True
)

"""Show class names of image dataset"""

class_names = dataset.class_names

# for image_batch, labels_batch in dataset.take(1):
#     print(image_batch.shape)
#     print(labels_batch.numpy())

"""### Visuliaze sample image"""

# plt.figure(figsize=(12,10))
# for image_batch, label_batch in dataset.take(1):
#   for i in range(15):
#     plt.subplot(3,5,i+1)
#     plt.imshow(image_batch[i].numpy().astype('uint8'))
#     plt.title(class_names[label_batch[i]])
#     plt.axis('off')
# plt.show()

"""### Split dataset into
1. Training
2. Validation
3. Testing
"""

# make function split data
def get_split_data(ds, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=10000):
  assert (train_split + test_split + val_split) == 1

  ds_size = len(ds)
  
  if shuffle:
    ds = ds.shuffle(shuffle_size, seed=12)

  train_size = int(train_split * ds_size)
  val_size = int(val_split * ds_size)

  train_ds = ds.take(train_size)
  val_ds = ds.skip(train_size).take(val_size)
  test_ds = ds.skip(train_size).skip(val_size)


  return train_ds, val_ds, test_ds

train_ds, val_ds, test_ds = get_split_data(dataset)

"""### Cache, Shuffle, Prefetch Data"""

train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)
val_ds = val_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)
test_ds = test_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)

"""### Rescale, Resize, and Data Augmentation"""

# rescale & resize
rescale_resize = tf.keras.Sequential([
    tf.keras.layers.experimental.preprocessing.Rescaling(1./255),
    tf.keras.layers.experimental.preprocessing.Resizing(IMAGE_SIZE, IMAGE_SIZE),
])

# data augmentation
data_augmentation = tf.keras.Sequential([
    tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal_and_vertical'),
    tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),
    tf.keras.layers.experimental.preprocessing.RandomContrast(0.3),
])

# applying data augmentation to train_ds
train_ds = train_ds.map(
    lambda x, y: (data_augmentation(x, training=True), y)
).prefetch(buffer_size=tf.data.AUTOTUNE)

"""### Model Building Architecture"""

input_shape = (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, CHANNELS)
n_classes = 4

model = models.Sequential([
    rescale_resize,
    layers.Conv2D(32, padding='same', kernel_size = (3,3), activation='relu', input_shape=input_shape),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, padding='same', kernel_size = (3,3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, padding='same', kernel_size = (3,3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, padding='same', kernel_size=(3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, padding='same', kernel_size=(3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, padding='same', kernel_size=(3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Dropout(0.2),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(n_classes, activation='softmax'),
])

model.build(input_shape=input_shape)

# show summary model
model.summary()

"""### Compile & Fit"""

# compiling model
model.compile(
    optimizer='adam',
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
    metrics=['accuracy']
)

# make callback function for avoid overfit
callbacks=tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)

# fit the model
history = model.fit(
    train_ds,
    validation_data=val_ds,
    batch_size=BATCH_SIZE,
    verbose=1,
    epochs=50,
    callbacks=[callbacks]
)

# evaluate model performance
model.evaluate(test_ds)

"""### Testing Data"""

# visualize performance model
# loss = history.history['loss']
# val_loss = history.history['val_loss']
# acc = history.history['accuracy']
# val_acc = history.history['val_accuracy']

# plt.figure(figsize=(10,8))
# plt.subplot(1,2,1)
# plt.plot(range(len(loss)), loss, label='Training Loss')
# plt.plot(range(len(loss)), val_loss, label='Validation Loss')
# plt.title('Training and Validation Loss')
# plt.legend()

# plt.subplot(1,2,2)
# plt.plot(range(len(loss)), acc, label='Training Accuracy')
# plt.plot(range(len(loss)), val_acc, label='Validation Accuracy')
# plt.title('Training and Validation Accuracy')
# plt.legend()

# plt.show()

# Run Prediction with sample image

# for image_batch, label_batch in dataset.take(1):
#   first_img = image_batch[0].numpy().astype('uint8')
#   first_label = label_batch[0].numpy()

#   plt.imshow(first_img)
#   print(f'actual label: {class_names[first_label]}')

#   predicted_label = model.predict(image_batch)
#   print(f'predicted label: {class_names[np.argmax(predicted_label[0])]}')

# write function for inference
def model_predict(model, img):
  img_array = tf.keras.preprocessing.image.img_to_array(images[i].numpy())
  img_array = tf.expand_dims(img_array, 0)

  predictions = model.predict(img_array)
  predicted_class = class_names[np.argmax(predictions[0])]
  confidence = round(100 * (np.max(predictions[0])), 2)

  return predicted_class, confidence

# run inference with sample images
# plt.figure(figsize=(15,15))
# for images, labels in dataset.take(1):
#   for i in range(15):
#     plt.subplot(3,5,i+1)
#     plt.imshow(images[i].numpy().astype('uint8'))
#     actual_label = class_names[labels[i]]
#     predicted_class, confidence = model_predict(model, images[i].numpy())
#     plt.title(f"Actual: {actual_label},\n Predicted: {predicted_class}.\n Confidence: {confidence}%")
#     plt.axis('off')
# plt.show()

"""### Saving Model"""

model_version=max([int(i) for i in os.listdir("./save_model") + [0]])+1
model.save(f"./save_model/{model_version}")
